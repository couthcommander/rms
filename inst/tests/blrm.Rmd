--- 
title: "`rms::blrm` Examples"
author: "Frank Harrell"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: no
    code_folding: show
    theme: yeti
description: "`blrm` Examples"
---

# Overview and Setup

`blrm` is for Bayesian binary and ordinal proportional odds logistic regression.  It is the analog of `rms::lrm` and for ordinal responses is intended for outcomes with up to a few dozen ordinal levels.  `blrm` uses precompiled Stan code written by Ben Goodrich of Columbia University.  To use `blrm` you must prepare by doing the following:

1. Install the `rstan` package
1. Run the following R commands to compile Stan code used by `rms` whenever one of the `.stan` files changes [here](https://github.com/harrelfe/stan)
```
require(rms)    # On my system: stancompiled='~/R/stan'
options(stancompiled='whatever directory to hold compiled Stan code')
stanCompile()
```
This will download and compile the `.stan` programs, and will abort if you did not install `rstan` first.  Each program takes about a minute to compile, and you **do not** need to do this step for each of your project directories.

It is recommended that you put something like `options(stancompiled='~/R/stan')` in your home directory's `.Rprofile` file.

```{r setup}
require(rms)
knitrSet(lang='markdown', w=7, h=7, fig.path='png/')
options(mc.cores = parallel::detectCores())   # use max # CPUs
options(stancompiled='~/R/stan', prType='html')
# Run the following once to store compiled Stan code in a central
# place for all projects
# stanCompile('~/R/stan')
```

# Example: 10-level Ordinal Outcome

Simulate a dataset with three predictors and one 10-level ordinal outcome.  Run the frequentist proportional odds model then the Bayesian one.

```{r sim,results='asis'}
set.seed(1)
n <- 500
x1 <- runif(n, -1, 1)
x2 <- runif(n, -1, 1)
x3 <- sample(0 : 1, n, TRUE)
y <- x1 + 0.5 * x2 + x3 + rnorm(n)
y <- as.integer(cut2(y, g=10))
dd <- datadist(x1, x2, x3); options(datadist='dd')
f <- lrm(y ~ x1 + pol(x2, 2) + x3, eps=1e-7) # eps to check against rstan
f
```

Before getting posterior distributions of parameters, use `rstan` to just get maximum likelihood estimates and compare them with those from `lrm`.  Do this for increasingly flat priors for the $\beta$s.  The default prior SD for `blrm` is 100.  Running `method='optimizing'` is a quick way to study the effect of priors on the posterior modes for non-intercepts.  It is also a good way to get a sense of the scaling of parameters used in the actual calculations.  Except for the last parameter in the model, prior standard deviations apply to the QR orthonormalized version of the design matrix.  The magnitude of maximum (penalized) maximum likelihood estimates on the QR transformed scale ($\theta$s) can be compared to magnitudes of parameter estimates on the original scale by comparing the `blrm` returned object `coefficients` with the returned object `theta` when `method='optimizing'`.

```{r stanmle}
for(psd in c(0.25, 1, 10, 100, 10000)) {
	cat('\nPrior SD:', psd, '\n')
	g <- blrm(y ~ x1 + pol(x2, 2) + x3, method='optimizing', priorsd=psd)
	cat('-2 log likelihood:', g$deviance, '\n')
	print(g$coefficients)
}
# Compare with ordinary MLEs and deviance
f$deviance
coef(f)

# For the last model fitted with blrm compare the point estimates on the
# QR decomposition scale with those on the original scale
g$theta
g$coefficients[10:13]
```

Fit the model with Dirichlet priors on intercepts and wide normal priors on the $\beta$s.  Show the model fit summary.  Note that the indexes of predictive discrimination/accuracy include 0.95 credible intervals.  In frequentist inference we pretend that quantities such as AUROC and $R^2$ are estimated without error, which is far from the case.

In several places you will see an index named `Symmetry`.  This is a measure of the symmetry of a posterior distribution.  Values farther from 1.0 indicate asymmetry, which indicates that the use of standard errors and the use of a normal approximation for the posterior distribution are not justified.  The symmetry index is the ratio of the gap between the posterior mean and the 0.95 quantile of the posterior distribution to the gap between the 0.05 quantile and the mean.

```{r blrmsim,results='asis'}
g <- blrm(y ~ x1 + pol(x2, 2) + x3)
g
```

```{r blrmStats}
# Show more detailed analysis of model performance measures
blrmStats(g, pl=TRUE)
```

Show basic Stan diagnostics. Intercepts are shifted from what is in `g` because of subtractions of covariate means before passing data to `rstan`.

```{r standx}
stanDxplot(g)
stanDx(g)
```

Here are the posterior distributions, calculated using kernel density estimates from posterior draws.  Posterior models, shown as vertical lines, are parameter values that maximize the log posterior density (using `rstan::optimizing` in the original model fit) so do not necessarily coincide with the peak of the kernel density estimates.

```{r stanpost}
plot(g)    # invokes rstan::stan_dens, defaults to showing betas

# Print frequentist side-by-side with Bayesian posterior mean, median, mode

cbind(MLE=coef(f), t(g$param))

# Compare covariance matrix of posterior draws with MLE
round(diag(vcov(f)) / diag(vcov(g)), 2)
range(vcov(f) / vcov(g))
```

Next show frequentist and Bayesian contrasts.  For the Bayesian contrast the point estimate is the posterior mean, and the 0.95 credible interval is computed.  Instead of a p-value, the posterior probability that the contrast is positive is computed.

```{r contrast}
contrast(f, list(x1=0, x3=1), list(x1=.25, x3=0))
contrast(g, list(x1=0, x3=1), list(x1=.25, x3=0))
```

Compute posterior probabilities for various assertions about unknown true parameter values.  The `PostF` function is a function generator that effectively evaluates the assertion to a 0/1 value and computes the mean of these binary values over posterior draws.   As is the case with inference about the quadratic effect of `x2` below, when the assertion does not evaluate to a binary 0/1 or logical `TRUE/FALSE` value, it is taken as a quantity that is derived from one or more model parameters, and a posterior density is drawn for the derived parameter.  We use that to get a posterior distribution on the vertex of the quadratic `x2` effect.

```{r postprobs}
P <- PostF(g, pr=TRUE)   # show new short legal R names
P(b3 > 0 & b1 > 1.5)
P(b3 > 0)
P(abs(b3) < 0.25)        # evidence for small |nonlinearity|
mean(g$draws[, 'x2^2'] > 0)    # longhand calculation
# Plot posterior distribution for the vertex of the quadratic x2 effect
# This distribution should be wide because the relationship is linear
# (true value of b3 is zero)
P(-b2 / (2 * b3))
```

```{r postprobs2}
# Recreate the P function using original parameter names
# (which may not be legal R name)
P <- PostF(g, name='orig')
P(`x2^2` > 0)
P(`x2^2` > 0 & x1 > 1.5)

# Remove rstan results from g when finished with trace plots
s1 <- format(object.size(g), 'MB')
g$rstan <- NULL
s2 <- format(object.size(g), 'MB')
cat('Before:', s1, '  After:', s2, '\n')
```

# Example Using the `support` Dataset with Restricted Cubic Splines

Turn to the `support` dataset and fit a binary logistic model to predict the probability of in-hospital death of critically ill adults.
`blrm` keeps posterior sampling efficient by orthonormalizing the design matrix before doing the sampling (this is done internally in the Stan code).  This allows for arbitrary collinearities, for example in the basis functions used in restricted cubic splines.  When there are such collinearities, expect to see some disagreements in estimates between `blrm` and `lrm`, because the latter does not do orthonormalization (only normalization to mean 0 variance 1).  Collinearity implies that there are many different solutions to the equations, all giving almost the same predicted values.

```{r support,results='asis'}
getHdata(support)
dd <- datadist(support); options(datadist='dd')
f <- lrm(hospdead ~ dzgroup + rcs(crea, 5) + rcs(meanbp, 5),
				 data=support, eps=1e-4, x=TRUE)
f
htmlVerbatim(Function(f))
g <- blrm(hospdead ~ dzgroup + rcs(crea, 5) + rcs(meanbp, 5), 
					x=TRUE, y=TRUE, data=support)
g
htmlVerbatim(Function(g))   # by default uses posterior mode parameter values
# To add an intercept use e.g. Function(g, intercept=coef(g, 'mode')[5])

htmlVerbatim(stanDx(g))
stanDxplot(g)
plot(g)
```
Show approximate relative explained variation (REV) and compare this with Wald statistics from the frequentist `lrm` model.  REV is less accurate the more the multivariate posterior distribution differs from a multivariate normal distribution.  On a given posterior draw, REV for a term in the model is the Wald $\chi^2$ statistic divided by the Wald statistic for the whole model.

```{r rev,results='asis'}
a <- anova(g)
plot(a)
anova(f)
```
Compute odds ratios over default inter-quartile ranges for continuous predictors, based on posterior mode parameters.  Also show 0.95 credible intervals.  Note that unlike the `print` method, the `plot` method for `summary` doesn't actually compute credible intervals, but approximates them by assuming normality and using the standard deviation of the posterior samples.  Compute the plot with the ordinary `lrm` result.

```{r summary, results='asis'}
s <- summary(g)
s
plot(s)
plot(summary(f))
```

Draw partial effect plots with 0.95 credible intervals.  Point estimates are posterior modes (which can be easily changed).

```{r peffect}
ggplot(Predict(g))
```

Draw a nomogram from posterior mode parameter values.

```{r nomogram}
p <- nomogram(g, fun=plogis, funlabel='P(death)')
plot(p)
```
For comparison here is a nomogram based on maximum likelihood estimates of parameters rather than posterior modes.

```{r nomfreq}
plot(nomogram(f, fun=plogis, funlabel='P(death)'))
```

# Longitudinal Data Examples: Random Effects

Let's generate some data with repeatedly measured outcome per subject where the outcome is binary and the random effects have a $N(0, 0.25^2)$ distribution.  500 subjects have 10 measurements each.

```{r re, results='asis'}
n <- 500   # subjects
set.seed(2)
re <- rnorm(n) * 0.25
X <- runif(n)   # baseline covariate, will be duplicated over repeats
m <- 10         # measurements per subject

id <- rep(1 : n, each = m)
x  <- X[id]
L <- x + re[id]   # actual logit
y <- ifelse(runif(n * m) <= plogis(L), 1, 0)
f <- lrm(y ~ x)     # ordinary fit
f     # now use cluster sandwich covariance estimator:
g <- robcov(f, id)  # covariance matrix adjusted for clustering
g
```

Now use a Bayesian random effects model.  The prior distribution for the standard deviation $\sigma_{\gamma}$ of the random effects ($\gamma$s) is assumed to be exponential, and we will use the default mean of this distribution of 1.0.

```{r bayesre, results='asis'}
b <- blrm(y ~ x + cluster(id))
b
plot(b)
```

```{r bayesreh}
# Plot distribution of the 500 estimated random effects (posterior medians)
hist(b$gammas, xlab='Estimated Random Effects', nclass=40)
```

Now generate similar data except for a bimodal random effects distribution.  This will fool the random effects normal prior into having a wider variance for a single normal distribution but will still result in estimated random effects that are somewhat realistic.

```{r re2, results='asis',w=7,h=6}
n <- 500
set.seed(3)
re <- c(rnorm(n/2, mean=-1.75), rnorm(n/2, mean=1.75)) * 0.25
cat('SD of real random effects:', round(sd(re), 4), '\n')
X <- runif(n)   # baseline covariate, will be duplicated over repeats
m <- 10         # measurements per subject

id <- rep(1 : n, each = m)
x  <- X[id]
L <- x + re[id]   # actual logit
y <- ifelse(runif(n * m) <= plogis(L), 1, 0)
b <- blrm(y ~ x + cluster(id))
b
par(mfrow=c(2, 2))
hist(b$gammas, xlab='Estimated Random Effects', nclass=40, main='')
hist(re,       xlab='Real Random Effects',      nclass=40, main='')
plot(re, b$gammas, xlab='Real', ylab='Estimated')
abline(a=0, b=1)
```

# Absorbing State in Mixed Effects Ordinal Regression

`blrm` is not designed to handle this situation but let's see how it performs.

For an ordinal outcome y=0, 1, 2, 3, 4, 5 suppose that y=5 represents an absorbing state such as death.  Suppose that subjects are observed for 10 days, and if death occurs within those days, all later values of y for that subject are set to 5.  Generate repeated outcomes under a $n(0, 0.25^2)$ random effects model with two treatments: `a` and `b`.  The `b:a` odds ratio is 0.65 and the cell probabilities are 0.3, 0.3, 0.1, 0.1, 0.1, 0.1 corresponding to y=0-5, when the random effect is zero.

```{r os}
# Generate data as if there is no absorbing state
n <- 1000
set.seed(6)
pa <- c(.3, .3, .1, .1, .1, .1)     # P(Y=0-5 | tx=a, random effect=0)
pb <- pomodm(p=pa, odds.ratio=0.65) # P(Y=0-5 | tx=b, re=0)   # Hmisc
round(pb, 3)

re <- rnorm(n) * 0.25
tx <- c(rep('a', n/2), rep('b', n/2))   # will be duplicated over repeats
m <- 10         # measurements per subject

id   <- rep(1 : n, each = m)
time <- rep(1 : m, n)
or   <- exp(log(0.65) * (tx[id] == 'b') + re[id])
y   <- integer(n * m)
for(j in 1 : (n * m)) {
  p    <- pomodm(p=pa, odds.ratio=or[j])
	y[j] <- sample(0:5, 1, p, replace=TRUE)
}
Tx <- tx[id]
table(Tx, y)
```

The first Bayesian proportional odds model fitted is the one that exactly matches the data generation model, as we have not yet imposed an absorbing state, so that outcomes with y < 5 can appear after a y=5 outcome for the subject.

```{r noabs,results='asis'}
b <- blrm(y ~ Tx + cluster(id))
b
```

Now assume that state y=5 is an absorbing state.  Change observations after the first y=5 within subject to also have y=5.

```{r absorb, results='asis'}
require(data.table)
g <- function(x) if(length(x)) min(x, na.rm=TRUE) else 99L
u <- data.table(id, time, Tx, y, key='id')
w <- u[, .(first=g(time[y == 5])), by=id]
d <- u[w]

# Show distribution of first time of y=5
table(d[time == 1, first])
# Set all observations after the first y=5 to also have y=5
z <- d
z[time > first, y:=5]
table(u$y); table(d$y); table(z$y)
bcf <- blrm(y ~ Tx + cluster(id), data=z)
bcf
```

Next we truncate patient records so that y=5 is not carried forward.

```{r nocarry,results='asis'}
zt <- z[time <= first]
bnc <- blrm(y ~ Tx + cluster(id), data=zt)
bnc
```

# Computing Environment

`r markupSpecs$html$session()`
